class SAGEConvWithEdges(torch.nn.Module):
    """
    This is an implementation of the GraphSage convolution that also takes into account edge features.
    Source:
        https://github.com/kkonevets/geo_detection/blob/9421a591123c380a1f232b6bff598cae8ff29a23/sage_conv.py
    """
    def __init__(self, in_channels, in_edge_channels, out_channels):
        super(SAGEConvWithEdges, self).__init__()

        self.node_mlp_rel = Linear(in_channels + in_edge_channels, out_channels)

    def forward(self, x, edge_index, edge_attr):
        row, col = edge_index
        x_row = x[row]

        edge_attr = torch.cat([x_row, edge_attr], 1)

        edge_attr = F.normalize(edge_attr)

        x = scatter(edge_attr, col, dim=0, dim_size=res_size, reduce='mean')
        x = self.node_mlp_rel(x)
        x = F.normalize(x)
        return x
    
    def reset_parameters(self):
        for l in self.modules():
            if type(l) == torch.nn.Linear:
                l.reset_parameters()

    def __repr__(self):
        return ' '.join(
            str([l.in_features, l.out_features]) for l in self.modules()
            if type(l) == torch.nn.Linear)

****************************************************************************************
****************************************************************************************

class IIDxBD(Dataset):
    def __init__(self,
                 root,
                 resnet_pretrained=False,
                 resnet_shared=False,
                 resnet_diff=True,
                 transform=None,
                 pre_transform=None) -> None:
        
        self.resnet_pretrained = resnet_pretrained
        self.resnet_shared = resnet_shared
        self.resnet_diff = resnet_diff
        self.xbd_path = xbd_path

        super(IIDxBD, self).__init__(root, transform, pre_transform)

    @property
    def raw_file_names(self) -> List:
        return []

    @property
    def processed_file_names(self) -> List[str]:
        return ['iid_data_guatemala-volcano.pt', 'iid_data_hurricane-florence.pt',
                'iid_data_hurricane-harvey.pt', 'iid_data_hurricane-matthew.pt',
                'iid_data_hurricane-michael.pt', 'iid_data_mexico-earthquake.pt',
                'iid_data_midwest-flooding.pt', 'iid_data_palu-tsunami.pt',
                'iid_data_santa-rosa-wildfire.pt', 'iid_data_socal-fire.pt']

    def process(self):
        resnet50 = load_feature_extractor(self.resnet_pretrained, self.resnet_shared, self.resnet_diff)
        resnet50 = resnet50.to(device)
        disaster_folders = os.listdir(self.xbd_path + '/train_bldgs/')

        self.annot_list = []

        for disaster in disaster_folders:
            if os.path.isfile(os.path.join(self.processed_dir, 'iid_data_{}.pt'.format(disaster))):
                continue
            x = []
            y = []
            coords = []
            split = []

            list_pre_images = disasters_dict[disaster + '_pre']
            list_post_images = disasters_dict[disaster + '_post']

            annotation_train = pd.read_csv(disasters_dict[disaster + '_labels'][0], index_col=0)
            annotation_train['split'] = ['train'] * annotation_train.shape[0]
            annotation_hold = pd.read_csv(disasters_dict[disaster + '_labels'][1], index_col=0)
            annotation_hold['split'] = ['hold'] * annotation_hold.shape[0]
            annotation_test = pd.read_csv(disasters_dict[disaster + '_labels'][2], index_col=0)
            annotation_test['split'] = ['test'] * annotation_test.shape[0]
            annotation = pd.concat((annotation_train, annotation_hold, annotation_test))
            self.annot_list.append(annotation.drop(annotation[annotation['class']=='un-classified'].index))

            pbar = tqdm(total=len(list_post_images))
            pbar.set_description(f'Building {disaster}, node features')

            for pre_image_file, post_image_file in zip(list_pre_images, list_post_images):
                if annotation.loc[os.path.split(post_image_file)[1],'class'] == 'un-classified':
                    continue

                #ordinal encoding of labels
                label = annotation.loc[os.path.split(post_image_file)[1],'class']
                if label == 'no-damage':
                    y.append([1,0,0,0])
                elif label == 'minor-damage':
                    y.append([1,1,0,0])
                elif label == 'major-damage':
                    y.append([1,1,1,0])
                elif label == 'destroyed':
                    y.append([1,1,1,1])
                else:
                    raise ValueError(f'Label class {label} undefined.')

                coords.append(annotation.loc[os.path.split(post_image_file)[1],'coords'])
                split.append(annotation.loc[os.path.split(post_image_file)[1],'split'])

                pre_image = Image.open(pre_image_file)
                post_image = Image.open(post_image_file)
                pre_image = pre_image.resize((256, 256))
                post_image = post_image.resize((256, 256))
                pre_image = transform(pre_image)
                post_image = transform(post_image)
                images = torch.cat((pre_image, post_image),0)
                images = images.to(device)
                with torch.no_grad():
                    node_features = resnet50(images.unsqueeze(0)).flatten()
                x.append(node_features.cpu())
                pbar.update()
            
            pbar.close()
            x = torch.stack(x)
            y = torch.tensor(y)

            #mask as train/val/test according to
            #https://stackoverflow.com/questions/65670777/loading-a-single-graph-into-a-pytorch-geometric-data-object-for-node-classificat
            train_mask = torch.zeros(x.shape[0])
            hold_mask = torch.zeros(x.shape[0])
            test_mask = torch.zeros(x.shape[0])

            split = pd.Series(split)
            train_mask[split[split=='train'].index] = 1
            hold_mask[split[split=='hold'].index] = 1
            test_mask[split[split=='test'].index] = 1

            train_mask = train_mask.type(torch.bool)
            hold_mask = hold_mask.type(torch.bool)
            test_mask = test_mask.type(torch.bool)

            #edge index matrix
            edge_index = build_edge_idx(x.shape[0])

            #edge features
            pbar = tqdm(total=edge_index.shape[1])
            pbar.set_description(f'Building {disaster}, edge features')
            
            edge_attr = torch.empty((edge_index.shape[1],2))
            for i in range(edge_index.shape[1]):
                node1 = x[edge_index[0,i]]
                node2 = x[edge_index[1,i]]
                coords1 = coords[edge_index[0,i]]
                coords2 = coords[edge_index[1,i]]
                attr = get_edge_weight(node1, node2, coords1, coords2)
                edge_attr[i,0] = attr[0]
                edge_attr[i,1] = attr[1]
                pbar.update()
            
            pbar.close()
            data = Data(x=x, edge_index=edge_index, edge_attr=edge_attr, y=y,
                        train_mask=train_mask, val_mask=test_mask, test_mask=hold_mask)

            if self.pre_filter is not None and not self.pre_filter(data):
                continue
            if self.pre_transform is not None:
                data = self.pre_transform(data)
            
            torch.save(data, os.path.join(self.processed_dir, 'iid_data_{}.pt'.format(disaster)))
    
    def len(self):
        return len(self.processed_file_names)

    def get(self, idx):
        data = torch.load(os.path.join(self.processed_dir, self.processed_file_names[idx]))
        return data

****************************************************************************************
****************************************************************************************

def generate_disaster_dict() -> None:
    xbd_path = 'datasets/xbd'
    disaster_dict = defaultdict(list)
    subsets = ('/train_bldgs/', '/hold_bldgs/', '/test_bldgs/', '/tier3_bldgs/')

    disaster_folders = os.listdir(xbd_path + subsets[0]) + os.listdir(xbd_path + subsets[-1])

    for subset in subsets:
        for disaster in disaster_folders:
            disaster_dict[disaster + '_pre'].extend(map(str, Path(xbd_path + subset + disaster).glob('*pre_disaster*')))
            disaster_dict[disaster + '_post'].extend(map(str, Path(xbd_path + subset + disaster).glob('*post_disaster*')))
            disaster_dict[disaster + '_labels'].extend(map(str, Path(xbd_path + subset + disaster).glob('*.csv')))

    with open('disaster_dirs.json', "w") as outfile: 
        json.dump(disaster_dict, outfile)

    print('Successfully created "disaster_dirs.json".')

****************************************************************************************
****************************************************************************************

def define_neigborhood(labels: pd.DataFrame):
    pairwise_euc = pairwise_distances(labels.values, metric=euclidean_similarity, njobs=-1)
    results = {}
    for idx, row in labels.iterrows():
        similar_indices = pairwise_euc[idx].argsort()
        similar_indices = similar_indices[:-len(similar_indices)-1:-1]
        similar_items = [(pairwise_euc[idx][i], labels['name'][i]) for i in similar_indices]
        results[row['id']] = similar_items[1:]

    norm = lambda row: sqrt(row[1]**2 + row[2]**2)
    labels['norm'] = labels.apply(norm, axis=1)
    labels.sort_values(by='norm', inplace=True)
    i = 0
    neighbors = defaultdict(list)
    while i < labels.shape[0]:
        neighbors[labels['name'][0]] = results[0][:7600]
    return

single_zones = labels['zone'].value_counts()[labels['zone'].value_counts()==1].index.tolist()
if len(single_zones) == 1:
    dict_[disaster+subset_marker+'_pre_'+zones[-1]].extend(map(str, Path(xbd_path + subset + disaster).glob(f'{single_zones[0]}*pre_disaster*')))
    dict_[disaster+subset_marker+'_post_'+zones[-1]].extend(map(str, Path(xbd_path + subset + disaster).glob(f'{single_zones[0]}*post_disaster*')))
else:
    new_zone = '&'.join(single_zone for single_zone in single_zones)
    for single_zone in single_zones:
        dict_[disaster+subset_marker+'_pre_'+new_zone].extend(map(str, Path(xbd_path + subset + disaster).glob(f'{single_zone}*pre_disaster*')))
        dict_[disaster+subset_marker+'_post_'+new_zone].extend(map(str, Path(xbd_path + subset + disaster).glob(f'{single_zone}*post_disaster*')))