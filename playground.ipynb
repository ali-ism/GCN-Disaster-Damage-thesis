{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "xbd_path = 'datasets/xbd'\n",
    "subsets = ('/train_bldgs/', '/hold_bldgs/', '/test_bldgs/', '/tier3_bldgs/')\n",
    "disaster_folders = os.listdir(xbd_path + subsets[0])\n",
    "\n",
    "i_subset = 0\n",
    "i_disaster = 5\n",
    "\n",
    "print(list(Path(xbd_path + subsets[i_subset] + disaster_folders[i_disaster]).glob('*.csv*'))[0])\n",
    "labels = pd.read_csv(list(Path(xbd_path + subsets[i_subset] + disaster_folders[i_disaster]).glob('*.csv*'))[0])\n",
    "labels.columns = ['name', 'xcoords', 'ycoords', 'long', 'lat', 'class']\n",
    "zone = lambda row: '_'.join(row['name'].split('_', 2)[:2])\n",
    "labels['zone'] = labels.apply(zone, axis=1)\n",
    "labels['zone'].value_counts()\n",
    "#labels['zone'].value_counts()[labels['zone'].value_counts()==1].index.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from visualization import plot_on_image\n",
    "\n",
    "plot_on_image(labels, subsets[i_subset], 'mexico-earthquake_00000192')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from visualization import plot_on_map\n",
    "\n",
    "plot_on_map(labels, mapbox=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from visualization import CmapString\n",
    "\n",
    "cmap = CmapString(palette='viridis', domain=labels['zone'].values)\n",
    "\n",
    "plt.figure(figsize=(12,8))\n",
    "for _, row in labels.iterrows():\n",
    "    plt.scatter(row['xcoords'], row['ycoords'], label=row['zone'], color=cmap.color(row['zone']))\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Graph Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "import json\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = 'datasets/xbd/hold_bldgs/'\n",
    "disaster_folders = os.listdir(path)\n",
    "disaster = 'mexico-earthquake'\n",
    "\n",
    "labels = pd.read_csv(list(Path(path + disaster).glob('*.csv*'))[0], index_col=0)\n",
    "labels.drop(columns=['long','lat'], inplace=True)\n",
    "zone = lambda row: '_'.join(row.name.split('_', 2)[:2])\n",
    "labels['zone'] = labels.apply(zone, axis=1)\n",
    "\n",
    "processed_files = []\n",
    "zones = labels['zone'].value_counts()[labels['zone'].value_counts()>1].index.tolist()\n",
    "for zone in zones:\n",
    "     if not ((labels[labels['zone'] == zone]['class'] == 'un-classified').all() or \\\n",
    "            (labels[labels['zone'] == zone]['class'] != 'un-classified').sum() == 1):\n",
    "        processed_files.append(f'{zone}.pt')\n",
    "\n",
    "len(processed_files)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "from torch_geometric.data import GraphSAINTNodeSampler\n",
    "from tqdm import tqdm\n",
    "from dataset import xBD\n",
    "from model import DeeperGCN\n",
    "from metrics import xview2_f1_score\n",
    "\n",
    "with open('exp_settings.json', 'r') as JSON:\n",
    "    settings_dict = json.load(JSON)\n",
    "\n",
    "seed = settings_dict['seed']\n",
    "batch_size = settings_dict['data']['batch_size']\n",
    "num_steps = settings_dict['data']['saint_num_steps']\n",
    "name = settings_dict['model']['name']\n",
    "\n",
    "train_set = settings_dict['train_set']\n",
    "if len(train_set) == 1:\n",
    "    if train_set[0] == 'mexico-earthquake':\n",
    "        train_root = settings_dict['data']['mexico_train_root']\n",
    "        test_root = settings_dict['data']['mexico_test_root']\n",
    "    else:\n",
    "        train_root = settings_dict['data']['palu_train_root']\n",
    "        test_root = settings_dict['data']['palu_test_root']\n",
    "else:\n",
    "    train_root = settings_dict['data']['palu_matthew_rosa_train_root']\n",
    "    test_root = settings_dict['data']['palu_matthew_rosa_test_root']\n",
    "hold_root = settings_dict['data']['mexico_hold_root']\n",
    "\n",
    "hidden_units = settings_dict['model']['hidden_units']\n",
    "num_layers = settings_dict['model']['num_layers']\n",
    "dropout_rate = settings_dict['model']['dropout_rate']\n",
    "lr = settings_dict['model']['lr']\n",
    "n_epochs = settings_dict['epochs']\n",
    "starting_epoch = settings_dict['starting_epoch']\n",
    "path = settings_dict['model']['path']\n",
    "save_best_only = settings_dict['save_best_only']\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "torch.manual_seed(seed)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train_dataset = xBD(train_root, 'train', train_set)\n",
    "#test_dataset = xBD(train_root, 'test', train_set)\n",
    "hold_dataset = xBD(hold_root, 'hold', ['mexico-earthquake'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = DeeperGCN(hold_dataset.num_node_features,\n",
    "                  hold_dataset.num_edge_features,\n",
    "                  hidden_units,\n",
    "                  hold_dataset.num_classes,\n",
    "                  num_layers,\n",
    "                  dropout_rate)\n",
    "model_path = path + '/' + name + '_best.pt'\n",
    "model.load_state_dict(torch.load(model_path))\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = hold_dataset[1]\n",
    "sampler = GraphSAINTNodeSampler(data, batch_size=batch_size, num_steps=num_steps, num_workers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Data(edge_attr=[243951, 2], edge_index=[2, 243951], x=[699, 131072], y=[699, 4])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for subdata in sampler:\n",
    "    subdata = subdata.to(device)\n",
    "    out = model(subdata.x, subdata.edge_index, subdata.edge_attr)\n",
    "    loss = F.binary_cross_entropy(input=out, target=subdata.y.float(), weight=torch.Tensor(class_weights))\n",
    "    print(loss)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    model.eval()\n",
    "    ys = []\n",
    "    outs = []\n",
    "    for subdata in sampler:\n",
    "        subdata = subdata.to(device)\n",
    "        outs.append(model(subdata.x, subdata.edge_index, subdata.edge_attr).cpu())\n",
    "        ys.append(subdata.y.cpu())\n",
    "    outs = torch.cat(outs)\n",
    "    ys = torch.cat(ys)\n",
    "    f1 = xview2_f1_score(ys, outs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0, 2])\n",
      "tensor([1012,    1])\n"
     ]
    }
   ],
   "source": [
    "from metrics import parse_ordinal_output\n",
    "unique, counts = torch.unique(parse_ordinal_output(ys), return_counts=True)\n",
    "print(unique)\n",
    "print(counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0])\n",
      "tensor([1013])\n"
     ]
    }
   ],
   "source": [
    "unique, counts = torch.unique(outs.argmax(dim=1), return_counts=True)\n",
    "print(unique)\n",
    "print(counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.999997999015862e-06"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.99950617 0.        ]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "y_true = outs.argmax(dim=1)\n",
    "y_pred = parse_ordinal_output(ys)\n",
    "f1_classes = f1_score(y_true, y_pred, average=None)\n",
    "print(f1_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.999997999015862e-06"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "epsilon = 1e-6\n",
    "len(f1_classes) / sum((f1+epsilon)**-1 for f1 in f1_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4997530864197531"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f1_score(y_true, y_pred, average='macro')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from metrics import to_onehot\n",
    "\n",
    "y = [[1,0,0,0],[1,1,0,0],[1,1,1,1]]\n",
    "y = torch.Tensor(y)\n",
    "\n",
    "to_onehot(y)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "95d70da30fd22c72e34c5b2686d9425669d156b01d561237a148d8f19599268b"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
