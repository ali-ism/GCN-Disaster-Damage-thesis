{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Data Visualization"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import pandas as pd\r\n",
    "from pathlib import Path\r\n",
    "\r\n",
    "xbd_path = 'datasets/xbd'\r\n",
    "subsets = ('/train_bldgs/', '/hold_bldgs/', '/test_bldgs/', '/tier3_bldgs/')\r\n",
    "disaster_folders = os.listdir(xbd_path + subsets[0])\r\n",
    "\r\n",
    "i_subset = 0\r\n",
    "i_disaster = 5\r\n",
    "\r\n",
    "print(list(Path(xbd_path + subsets[i_subset] + disaster_folders[i_disaster]).glob('*.csv*'))[0])\r\n",
    "labels = pd.read_csv(list(Path(xbd_path + subsets[i_subset] + disaster_folders[i_disaster]).glob('*.csv*'))[0])\r\n",
    "labels.columns = ['name', 'xcoords', 'ycoords', 'long', 'lat', 'class']\r\n",
    "zone = lambda row: '_'.join(row['name'].split('_', 2)[:2])\r\n",
    "labels['zone'] = labels.apply(zone, axis=1)\r\n",
    "labels['zone'].value_counts()\r\n",
    "#labels['zone'].value_counts()[labels['zone'].value_counts()==1].index.tolist()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from visualization import plot_on_image\r\n",
    "\r\n",
    "plot_on_image(labels, subsets[i_subset], 'mexico-earthquake_00000192')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from visualization import plot_on_map\r\n",
    "\r\n",
    "plot_on_map(labels, mapbox=False)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import matplotlib\r\n",
    "import matplotlib.pyplot as plt\r\n",
    "import numpy as np\r\n",
    "from visualization import CmapString\r\n",
    "\r\n",
    "cmap = CmapString(palette='viridis', domain=labels['zone'].values)\r\n",
    "\r\n",
    "plt.figure(figsize=(12,8))\r\n",
    "for _, row in labels.iterrows():\r\n",
    "    plt.scatter(row['xcoords'], row['ycoords'], label=row['zone'], color=cmap.color(row['zone']))\r\n",
    "plt.axis('off')\r\n",
    "plt.show()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "from PIL import Image\r\n",
    "from torchvision.transforms import ToTensor\r\n",
    "\r\n",
    "tr = ToTensor()\r\n",
    "\r\n",
    "image_pre = Image.open(\"D:/thesis/datasets/socal-fire_00000012_pre_disaster_2.png\").resize((128,128))\r\n",
    "image_post = Image.open(\"D:/thesis/datasets/socal-fire_00000124_post_disaster_4.png\").resize((128,128))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "import torch\r\n",
    "\r\n",
    "image_pre = tr(image_pre)\r\n",
    "image_post = tr(image_post)\r\n",
    "images = torch.cat((image_pre, image_post),0)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "images.shape"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "imgs = images.flatten().unsqueeze(dim=0)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "from model import SiameseResnetEncoder\r\n",
    "\r\n",
    "model = SiameseResnetEncoder()\r\n",
    "model(imgs).shape"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "torch.Size([1, 2048])"
      ]
     },
     "metadata": {},
     "execution_count": 4
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "newimgs = imgs.reshape((-1, 6, 128, 128))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "newimgs.shape"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import matplotlib.pyplot as plt\r\n",
    "\r\n",
    "plt.imshow(newimgs[0,:3,:,:].permute(1,2,0))\r\n",
    "plt.show()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "plt.imshow(newimgs[0,3:,:,:].permute(1,2,0))\r\n",
    "plt.show()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "---\n",
    "\n",
    "# Graph Generation"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import os\r\n",
    "from pathlib import Path\r\n",
    "import json\r\n",
    "import pandas as pd\r\n",
    "\r\n",
    "path = 'datasets/xbd/train_bldgs/'\r\n",
    "disaster_folders = os.listdir(path)\r\n",
    "disaster = 'socal-fire'\r\n",
    "\r\n",
    "labels = pd.read_csv(list(Path(path + disaster).glob('*.csv*'))[0], index_col=0)\r\n",
    "labels.drop(columns=['long','lat'], inplace=True)\r\n",
    "zone = lambda row: '_'.join(row.name.split('_', 2)[:2])\r\n",
    "labels['zone'] = labels.apply(zone, axis=1)\r\n",
    "\r\n",
    "processed_files = []\r\n",
    "zones = labels['zone'].value_counts()[labels['zone'].value_counts()>1].index.tolist()\r\n",
    "for zone in zones:\r\n",
    "     if not ((labels[labels['zone'] == zone]['class'] == 'un-classified').all() or \\\r\n",
    "            (labels[labels['zone'] == zone]['class'] != 'un-classified').sum() == 1):\r\n",
    "        processed_files.append(f'{zone}.pt')\r\n",
    "\r\n",
    "len(processed_files)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "for zone in [zones[0]]:\r\n",
    "    if (labels[labels['zone'] == zone]['class'] == 'un-classified').all() or \\\r\n",
    "    (labels[labels['zone'] == zone]['class'] != 'un-classified').sum() == 1:\r\n",
    "        continue\r\n",
    "    print(f'Building {zone}...')\r\n",
    "    list_pre_images = list(map(str, Path(path + disaster).glob(f'{zone}_pre_disaster*')))\r\n",
    "    list_post_images = list(map(str, Path(path + disaster).glob(f'{zone}_post_disaster*')))\r\n",
    "    coords = []\r\n",
    "\r\n",
    "    for pre_image_file, post_image_file in zip(list_pre_images, list_post_images):\r\n",
    "        \r\n",
    "        annot = labels.loc[os.path.split(post_image_file)[1],'class']\r\n",
    "        if annot == 'un-classified':\r\n",
    "            continue\r\n",
    "        coords.append((labels.loc[os.path.split(post_image_file)[1],'xcoord'],\r\n",
    "                        labels.loc[os.path.split(post_image_file)[1],'ycoord']))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "---\r\n",
    "\r\n",
    "# Graph Connectivity"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import torch\r\n",
    "import networkx as nx\r\n",
    "from torch_geometric.utils import to_networkx\r\n",
    "\r\n",
    "data = torch.load('datasets/socal-fire_00000908.pt')\r\n",
    "data.pos = torch.tensor(coords)\r\n",
    "datax = to_networkx(data)\r\n",
    "pos = dict(enumerate(data.pos.numpy()))\r\n",
    "pos = {node: (x,-y) for (node, (x,y)) in pos.items()}\r\n",
    "nx.draw_networkx(datax, pos=pos, arrows=False, with_labels=False, node_size=20, node_color='red')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from visualization import plot_on_image\r\n",
    "plot_on_image(labels, 'train', 'socal-fire_00000908')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "data"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from torch_geometric.transforms import Compose, Delaunay, FaceToEdge\r\n",
    "trans = Compose([Delaunay(), FaceToEdge()])\r\n",
    "datad = trans(data)\r\n",
    "datad"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "datadx = to_networkx(datad)\r\n",
    "nx.draw_networkx(datadx, pos=pos, arrows=False, with_labels=False, node_size=20, node_color='red')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "---\n",
    "\n",
    "# Training"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import json\r\n",
    "import numpy as np\r\n",
    "import matplotlib.pyplot as plt\r\n",
    "import torch\r\n",
    "import torch.nn.functional as F\r\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\r\n",
    "from torch_geometric.data import RandomNodeSampler, GraphSAINTNodeSampler\r\n",
    "from tqdm import tqdm\r\n",
    "from dataset import xBD\r\n",
    "from model import DeeperGCN\r\n",
    "from metrics import score\r\n",
    "from utils import get_class_weights\r\n",
    "\r\n",
    "with open('exp_settings.json', 'r') as JSON:\r\n",
    "    settings_dict = json.load(JSON)\r\n",
    "\r\n",
    "seed = 42\r\n",
    "batch_size = settings_dict['data']['batch_size']\r\n",
    "num_steps = settings_dict['data']['saint_num_steps']\r\n",
    "name = settings_dict['model']['name']\r\n",
    "train_set = settings_dict['train_set']\r\n",
    "if len(train_set) == 1:\r\n",
    "    if train_set[0] == 'socal-fire':\r\n",
    "        train_root = \"/home/ami31/scratch/datasets/pixel/socal_train\"\r\n",
    "    else:\r\n",
    "        train_root = \"/home/ami31/scratch/datasets/pixel/sunda\"\r\n",
    "else:\r\n",
    "    train_root = \"/home/ami31/scratch/datasets/pixel/sunda_tucaloosa_puna\"\r\n",
    "test_root = \"/home/ami31/scratch/datasets/pixel/socal_test\"\r\n",
    "hold_root = \"/home/ami31/scratch/datasets/pixel/socal_hold\"\r\n",
    "hidden_units = settings_dict['model']['hidden_units']\r\n",
    "num_layers = settings_dict['model']['num_layers']\r\n",
    "dropout_rate = settings_dict['model']['dropout_rate']\r\n",
    "lr = settings_dict['model']['lr']\r\n",
    "n_epochs = settings_dict['epochs']\r\n",
    "starting_epoch = settings_dict['starting_epoch']\r\n",
    "path = settings_dict['model']['path']\r\n",
    "save_best_only = settings_dict['save_best_only']\r\n",
    "\r\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\r\n",
    "torch.manual_seed(seed)\r\n",
    "torch.backends.cudnn.deterministic = True\r\n",
    "torch.backends.cudnn.benchmark = False"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "train_dataset = xBD(train_root, 'train', train_set).shuffle()\r\n",
    "test_dataset = xBD(test_root, 'test', train_set)\r\n",
    "hold_dataset = xBD(hold_root, 'hold', ['socal-fire'])"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "y_all = [data.y for data in hold_dataset]\r\n",
    "y_all = torch.cat(y_all)\r\n",
    "torch.unique(y_all, return_counts=True)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "model = DeeperGCN(hold_dataset.num_node_features,\r\n",
    "                  hold_dataset.num_edge_features,\r\n",
    "                  hidden_units,\r\n",
    "                  hold_dataset.num_classes,\r\n",
    "                  num_layers,\r\n",
    "                  dropout_rate)\r\n",
    "#model_path = path + '/' + name + '_best.pt'\r\n",
    "#model.load_state_dict(torch.load(model_path))\r\n",
    "model = model.to(device)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "model.train()\r\n",
    "class_weights = get_class_weights(train_set, train_dataset)\r\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr)\r\n",
    "pbar = tqdm(total=len(train_dataset))\r\n",
    "total_loss = 0\r\n",
    "for data in train_dataset[:5]:\r\n",
    "    if data.num_nodes > batch_size:\r\n",
    "        sampler = GraphSAINTNodeSampler(data, batch_size=batch_size, num_steps=num_steps, num_workers=2)\r\n",
    "        data_loss = 0\r\n",
    "        total_examples = 0\r\n",
    "        for subdata in sampler:\r\n",
    "            subdata = subdata.to(device)\r\n",
    "            optimizer.zero_grad()\r\n",
    "            out = model(subdata.x, subdata.edge_index, subdata.edge_attr)\r\n",
    "            loss = F.nll_loss(input=out, target=subdata.y, weight=class_weights.to(device))\r\n",
    "            loss.backward()\r\n",
    "            optimizer.step()\r\n",
    "            data_loss += loss.item() * subdata.num_nodes\r\n",
    "            total_examples += subdata.num_nodes\r\n",
    "        total_loss += data_loss / total_examples\r\n",
    "    else:\r\n",
    "        data = data.to(device)\r\n",
    "        optimizer.zero_grad()\r\n",
    "        out = model(data.x, data.edge_index, data.edge_attr)\r\n",
    "        loss = F.nll_loss(input=out, target=data.y, weight=class_weights.to(device))\r\n",
    "        loss.backward()\r\n",
    "        optimizer.step()\r\n",
    "        total_loss += loss.item()\r\n",
    "    pbar.update()\r\n",
    "pbar.close()\r\n",
    "print(total_loss / len(train_dataset))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "---"
   ],
   "metadata": {}
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "95d70da30fd22c72e34c5b2686d9425669d156b01d561237a148d8f19599268b"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.11 64-bit"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.11",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}