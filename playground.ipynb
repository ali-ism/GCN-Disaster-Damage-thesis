{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "xbd_path = 'datasets/xbd'\n",
    "subsets = ('/train_bldgs/', '/hold_bldgs/', '/test_bldgs/', '/tier3_bldgs/')\n",
    "disaster_folders = os.listdir(xbd_path + subsets[0])\n",
    "labels = []\n",
    "\n",
    "for subset in subsets[:3]:\n",
    "    for disaster in disaster_folders:\n",
    "        label = pd.read_csv(list(Path(xbd_path + subset + disaster).glob('*.csv*'))[0])\n",
    "        label.drop(index=label[label['class']=='un-classified'].index, inplace=True)\n",
    "        label.columns = ['name', 'xcoords', 'ycoords', 'long', 'lat', 'class']\n",
    "        label['disaster'] = disaster + '_' + re.findall(r\"/\\w+_\", subset)[0][1:-1]\n",
    "        zone = lambda row: '_'.join(row['name'].split('_', 2)[:2])\n",
    "        label['zone'] = label.apply(zone, axis=1)\n",
    "        labels.append(label)\n",
    "\n",
    "labels = pd.concat(labels)\n",
    "#labels.to_csv('datasets/tier1.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('datasets/tier3.csv', index_col=0)\n",
    "df['class_num'] = df['class'].map({'no-damage':0,'minor-damage':1,'major-damage':2,'destroyed':3})\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.groupby('disaster').apply(lambda x: x.groupby('zone')['class_num'].sum().apply(lambda x: 0 if x==0 else 1)).to_csv('datasets/tier3_zone.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from visualization import plot_on_map\n",
    "\n",
    "plot_on_map(labels, mapbox=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Polygon Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from visualization import plot_image\n",
    "\n",
    "plot_image(\"D:/thesis/datasets/xbd/train/labels/mexico-earthquake_00000098_post_disaster.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Image Reshaping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "from torchvision.transforms import ToTensor\n",
    "\n",
    "tr = ToTensor()\n",
    "\n",
    "image_pre = Image.open(\"D:/thesis/datasets/socal-fire_00000012_pre_disaster_2.png\").resize((128,128))\n",
    "image_post = Image.open(\"D:/thesis/datasets/socal-fire_00000124_post_disaster_4.png\").resize((128,128))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "image_pre = tr(image_pre)\n",
    "image_post = tr(image_post)\n",
    "images = torch.cat((image_pre, image_post),0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imgs = images.flatten().unsqueeze(dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "newimgs = imgs.reshape((-1, 6, 128, 128))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.imshow(newimgs[0,:3,:,:].permute(1,2,0))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(newimgs[0,3:,:,:].permute(1,2,0))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Graph Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "path = 'datasets/xbd/train_bldgs/'\n",
    "disaster_folders = os.listdir(path)\n",
    "disaster = 'socal-fire'\n",
    "\n",
    "labels = pd.read_csv(list(Path(path + disaster).glob('*.csv*'))[0], index_col=0)\n",
    "labels.drop(columns=['long','lat'], inplace=True)\n",
    "zone = lambda row: '_'.join(row.name.split('_', 2)[:2])\n",
    "labels['zone'] = labels.apply(zone, axis=1)\n",
    "\n",
    "processed_files = []\n",
    "zones = labels['zone'].value_counts()[labels['zone'].value_counts()>1].index.tolist()\n",
    "for zone in zones:\n",
    "     if not ((labels[labels['zone'] == zone]['class'] == 'un-classified').all() or \\\n",
    "            (labels[labels['zone'] == zone]['class'] != 'un-classified').sum() == 1):\n",
    "        processed_files.append(f'{zone}.pt')\n",
    "\n",
    "len(processed_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for zone in [zones[0]]:\n",
    "    if (labels[labels['zone'] == zone]['class'] == 'un-classified').all() or \\\n",
    "    (labels[labels['zone'] == zone]['class'] != 'un-classified').sum() == 1:\n",
    "        continue\n",
    "    print(f'Building {zone}...')\n",
    "    list_pre_images = list(map(str, Path(path + disaster).glob(f'{zone}_pre_disaster*')))\n",
    "    list_post_images = list(map(str, Path(path + disaster).glob(f'{zone}_post_disaster*')))\n",
    "    coords = []\n",
    "\n",
    "    for pre_image_file, post_image_file in zip(list_pre_images, list_post_images):\n",
    "        \n",
    "        annot = labels.loc[os.path.split(post_image_file)[1],'class']\n",
    "        if annot == 'un-classified':\n",
    "            continue\n",
    "        coords.append((labels.loc[os.path.split(post_image_file)[1],'xcoord'],\n",
    "                        labels.loc[os.path.split(post_image_file)[1],'ycoord']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Graph Connectivity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import networkx as nx\n",
    "from torch_geometric.utils import to_networkx\n",
    "\n",
    "data = torch.load('datasets/socal-fire_00000908.pt')\n",
    "data.pos = torch.tensor(coords)\n",
    "datax = to_networkx(data)\n",
    "pos = dict(enumerate(data.pos.numpy()))\n",
    "pos = {node: (x,-y) for (node, (x,y)) in pos.items()}\n",
    "nx.draw_networkx(datax, pos=pos, arrows=False, with_labels=False, node_size=20, node_color='red')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from visualization import plot_on_image\n",
    "plot_on_image(labels, 'train', 'socal-fire_00000908')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.transforms import Compose, Delaunay, FaceToEdge\n",
    "trans = Compose([Delaunay(), FaceToEdge()])\n",
    "datad = trans(data)\n",
    "datad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datadx = to_networkx(datad)\n",
    "nx.draw_networkx(datadx, pos=pos, arrows=False, with_labels=False, node_size=20, node_color='red')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from autoencoder import _make_cost_m\n",
    "from scipy.optimize import linear_sum_assignment\n",
    "\n",
    "cm = np.array([[46,26,475,26,11],[5,445,31,108,9],[4,506,32,54,1],[164,31,43,158,197],[408,12,6,15,123]])\n",
    "cm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "indexes = linear_sum_assignment(_make_cost_m(cm))\n",
    "indexes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm2 = cm[:, indexes[1]]\n",
    "cm2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#FP\n",
    "cm2.sum(axis=0) - np.diag(cm2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#FN\n",
    "cm2.sum(axis=1) - np.diag(cm2)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "74cf116ea1e63dd6a3edb90b75799d82a2f782fbf42bf6e53914daa55fad93a1"
  },
  "kernelspec": {
   "display_name": "Python 3.8.12 64-bit ('torch-geo': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
